#!/usr/bin/env python3
"""
[RESTORED] GENESIS MODULE - COMPLEXITY HIERARCHY ENFORCED
Original: c:\Users\patra\Genesis FINAL TRY\modules\reactive\live_alert_bridge.py
Hash: 5d7466078549a91526dfa586db1dbb3ee95134915dfeca81a23838b01deb2376
Type: PREFERRED
Restored: 2025-06-19T12:08:20.417733+00:00
Architect Compliance: VERIFIED
"""


# -*- coding: utf-8 -*-

"""
<!-- @GENESIS_MODULE_START: live_alert_bridge -->

# ╔════════════════════════════════════════════════════════╗
# ║      GENESIS AI TRADING SYSTEM - PHASE 18 MODULE      ║
# ║                LIVE ALERT BRIDGE v2.7                 ║
# ╚════════════════════════════════════════════════════════╝

Live Alert Bridge - GENESIS Reactive Execution Layer
Bridges all reactive execution confirmations and emergencies
to structured logging, alert generation, and future notification systems.

ARCHITECT MODE v2.7 COMPLIANCE:
- ✅ Event-driven (EventBus only)
- ✅ Real MT5 execution data logging
- ✅ Production-hardened threading
- ✅ Emergency priority queuing
- ✅ No real data, no local calls

INPUT EVENTS:
- StrategyFreezeConfirmed
- MacroSyncCompleted
- Emergency events from reactor/responder modules

OUTPUT ACTIONS:
- Structured logging to /logs/alert_bridge/
- Emergency alerts to /data/emergency_alerts/
- Reaction history to /data/reaction_history/
- Future webhook/Telegram integration hooks

<!-- @GENESIS_MODULE_END: live_alert_bridge -->
"""

import json
import os
import logging
import threading
import time
from datetime import datetime, timezone
from collections import defaultdict, deque
from typing import Dict, Any, Optional, List, Union
import queue
from pathlib import Path
from enum import Enum
import hashlib

# Import EventBus for communication
try:
    from event_bus import EventBus
except ImportError:
    import sys
    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
    from event_bus import EventBus


class AlertPriority(Enum):
    """Alert priority levels"""
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4
    EMERGENCY = 5


class LiveAlertBridge:
    """
    GENESIS Live Alert Bridge - Phase 18
    
    Bridges reactive execution events to alerting systems by:
    1. Processing all confirmation and emergency events
    2. Generating structured alerts with priority queuing
    3. Maintaining comprehensive reaction history
    4. Providing hooks for future notification integrations
    """
    
    def __init__(self):
        self.module_name = "LiveAlertBridge"
        self.version = "2.7"
        self.architect_mode = True
        
        # Thread-safe components
        self.lock = threading.RLock()
        self.alert_queue = queue.PriorityQueue(maxsize=10000)
        self.running = False
        self.worker_thread = None
        
        # Alert tracking
        self.alert_stats = defaultdict(int)
        self.alert_history = deque(maxlen=5000)
        self.emergency_alerts = deque(maxlen=1000)
        self.notification_queue = deque(maxlen=2000)
        
        # Priority and rate limiting
        self.priority_thresholds = {
            AlertPriority.EMERGENCY: 0,    # No delay
            AlertPriority.CRITICAL: 1,     # 1 second delay
            AlertPriority.HIGH: 5,         # 5 second delay
            AlertPriority.MEDIUM: 15,      # 15 second delay
            AlertPriority.LOW: 60          # 60 second delay
        }
        
        self.last_alert_times = defaultdict(float)
        self.alert_deduplication = {}
        self.deduplication_window = 300  # 5 minutes
        
        # Setup logging and directories
        self.setup_logging()
        self.setup_directories()
        
        # EventBus integration
        self.event_bus = EventBus()
        self.register_event_handlers()
        
        # Future integration hooks (placeholders)
        self.webhook_enabled = False
        self.telegram_enabled = False
        self.email_enabled = False
        self.discord_enabled = False
        
        self.log_bridge_startup()
    
    def setup_logging(self):
        """Configure production-grade logging"""
        log_dir = Path("logs/alert_bridge")
        log_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = logging.getLogger(f"GENESIS.{self.module_name}")
        self.logger.setLevel(logging.INFO)
        
        # File handler for structured logs
        log_file = log_dir / f"alerts_{datetime.now().strftime('%Y%m%d')}.jsonl"
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        
        # JSON formatter for structured logging
        formatter = logging.Formatter(
            '%(asctime)s|%(name)s|%(levelname)s|%(message)s'
        )
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)
        
        # Emergency alert file handler
        emergency_log = log_dir / f"emergency_{datetime.now().strftime('%Y%m%d')}.jsonl"
        emergency_handler = logging.FileHandler(emergency_log)
        emergency_handler.setLevel(logging.CRITICAL)
        emergency_handler.setFormatter(formatter)
        self.emergency_logger = logging.getLogger(f"GENESIS.{self.module_name}.EMERGENCY")
        self.emergency_logger.addHandler(emergency_handler)
        
        # Console handler for immediate feedback
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.WARNING)
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)
    
    def setup_directories(self):
        """Ensure all required directories exist"""
        directories = [
            "logs/alert_bridge",
            "data/emergency_alerts",
            "data/reaction_history",
            "data/alert_stats",
            "data/notification_queue",
            "data/webhook_logs",
            "data/telegram_logs"
        ]
        
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
    
    def register_event_handlers(self):
        """Register EventBus event handlers - NO LOCAL CALLS"""
        try:
            # Input event subscriptions for confirmations
            self.event_bus.subscribe("StrategyFreezeConfirmed", self.on_strategy_freeze_confirmed)
            self.event_bus.subscribe("MacroSyncCompleted", self.on_macro_sync_completed)
            self.event_bus.subscribe("TradeAdjustmentExecuted", self.on_trade_adjustment_executed)
            
            # Emergency event subscriptions
            self.event_bus.subscribe("EmergencyAlert", self.on_emergency_alert)
            self.event_bus.subscribe("KillSwitchTrigger", self.on_kill_switch_alert)
            self.event_bus.subscribe("ExecutionDeviationAlert", self.on_execution_deviation_alert)
            
            # System event subscriptions
            self.event_bus.subscribe("ResponderTerminationComplete", self.on_responder_termination)
            self.event_bus.subscribe("ReactorShutdown", self.on_reactor_shutdown)
            
            self.logger.info("EventBus handlers registered successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to register EventBus handlers: {e}")
            raise
    
    def log_bridge_startup(self):
        """Log bridge initialization"""
        startup_log = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "module": self.module_name,
            "version": self.version,
            "architect_mode": self.architect_mode,
            "event": "BRIDGE_STARTUP",
            "status": "INITIALIZED",
            "thread_safe": True,
            "real_data_only": True,
            "alert_priorities": {p.name: t for p, t in self.priority_thresholds.items()},
            "integration_hooks": {
                "webhook": self.webhook_enabled,
                "telegram": self.telegram_enabled,
                "email": self.email_enabled,
                "discord": self.discord_enabled
            }
        }
        
        self.logger.info(json.dumps(startup_log))
        
        # Save startup metrics
        stats_file = Path("data/alert_stats/startup_log.json")
        with open(stats_file, 'w') as f:
            json.dump(startup_log, f, indent=2)
    
    def start(self):
        """Start the alert bridge service"""
        with self.lock:
            if self.running:
                self.logger.warning("Alert bridge already running")
                return
            
            self.running = True
            self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
            self.worker_thread.start()
            
            self.logger.info("LiveAlertBridge started successfully")
    
    def stop(self):
        """Stop the alert bridge service"""
        with self.lock:
            if not self.running:
                return
            
            self.running = False
            
            # Send termination signal
            self.alert_queue.put((AlertPriority.LOW.value, time.time(), {"type": "SHUTDOWN"}))
            
            # Wait for safe termination
            if self.worker_thread and self.worker_thread.is_alive():
                self.worker_thread.join(timeout=5.0)
            
            # Flush remaining alerts
            self._flush_remaining_alerts()
            
            self.logger.info("LiveAlertBridge stopped")
    
    def _worker_loop(self):
        """Main worker thread for processing alerts"""
        self.logger.info("Alert bridge worker loop started")
        
        while self.running:
            try:
                # Process alerts with timeout
                try:
                    priority, timestamp, alert = self.alert_queue.get(timeout=1.0)
                    
                    if alert.get("type") == "SHUTDOWN":
                        break
                    
                    self._process_alert(priority, alert)
                    
                except queue.Empty:
                    continue
                    
            except Exception as e:
                self.logger.error(f"Worker loop error: {e}")
                time.sleep(1.0)
        
        self.logger.info("Alert bridge worker loop terminated")
    
    def _process_alert(self, priority: int, alert: Dict[str, Any]):
        """Process a single alert with priority handling"""
        try:
            alert_type = alert.get("alert_type", "unknown")
            alert_data = alert.get("data", {})
            
            # Check rate limiting based on priority
            if not self._check_rate_limit(priority, alert_type):
                self.logger.debug(f"Alert rate limited: {alert_type}")
                return
            
            # Check deduplication
            if self._is_duplicate_alert(alert):
                self.logger.debug(f"Duplicate alert filtered: {alert_type}")
                return
            
            # Process alert based on priority
            if priority <= AlertPriority.EMERGENCY.value:
                self._handle_emergency_alert(alert)
            elif priority <= AlertPriority.CRITICAL.value:
                self._handle_critical_alert(alert)
            else:
                self._handle_standard_alert(alert)
            
            # Update alert statistics
            with self.lock:
                self.alert_stats[alert_type] += 1
                self.alert_history.append(alert)
            
            # Save alert to reaction history
            self._save_alert_record(alert)
            
        except Exception as e:
            self.logger.error(f"Alert processing error: {e}")
    
    def _handle_emergency_alert(self, alert: Dict[str, Any]):
        """Handle emergency priority alerts"""
        try:
            alert_enriched = {
                **alert,
                "priority": "EMERGENCY",
                "processed_at": datetime.now(timezone.utc).isoformat(),
                "requires_immediate_attention": True
            }
            
            # Log to emergency logger
            self.emergency_logger.critical(json.dumps(alert_enriched))
            
            # Save to emergency alerts directory
            emergency_file = self._generate_emergency_filename(alert)
            emergency_path = Path("data/emergency_alerts") / emergency_file
            with open(emergency_path, 'w') as f:
                json.dump(alert_enriched, f, indent=2)
            
            # Add to emergency queue
            with self.lock:
                self.emergency_alerts.append(alert_enriched)
            
            # Future integration: Send immediate notifications
            self._queue_immediate_notification(alert_enriched)
            
            self.logger.critical(f"EMERGENCY ALERT PROCESSED: {alert.get('alert_type')}")
            
        except Exception as e:
            self.logger.error(f"Emergency alert handling error: {e}")
    
    def _handle_critical_alert(self, alert: Dict[str, Any]):
        """Handle critical priority alerts"""
        try:
            alert_enriched = {
                **alert,
                "priority": "CRITICAL",
                "processed_at": datetime.now(timezone.utc).isoformat(),
                "requires_attention": True
            }
            
            # Log to main logger
            self.logger.critical(json.dumps(alert_enriched))
            
            # Save to alerts directory
            alert_file = self._generate_alert_filename(alert, "critical")
            alert_path = Path("data/emergency_alerts") / alert_file
            with open(alert_path, 'w') as f:
                json.dump(alert_enriched, f, indent=2)
            
            # Future integration: Send notifications
            self._queue_notification(alert_enriched)
            
            self.logger.warning(f"CRITICAL ALERT PROCESSED: {alert.get('alert_type')}")
            
        except Exception as e:
            self.logger.error(f"Critical alert handling error: {e}")
    
    def _handle_standard_alert(self, alert: Dict[str, Any]):
        """Handle standard priority alerts"""
        try:
            alert_enriched = {
                **alert,
                "priority": "STANDARD",
                "processed_at": datetime.now(timezone.utc).isoformat(),
                "informational": True
            }
            
            # Log to main logger
            self.logger.info(json.dumps(alert_enriched))
            
            # Save to reaction history
            history_file = self._generate_alert_filename(alert, "standard")
            history_path = Path("data/reaction_history") / history_file
            with open(history_path, 'w') as f:
                json.dump(alert_enriched, f, indent=2)
            
            # Future integration: Queue for batch notifications
            self._queue_batch_notification(alert_enriched)
            
        except Exception as e:
            self.logger.error(f"Standard alert handling error: {e}")
    
    def _check_rate_limit(self, priority: int, alert_type: str) -> bool:
        """Check if alert passes rate limiting"""
        current_time = time.time()
        
        # Get threshold for priority level
        threshold = self.priority_thresholds.get(
            AlertPriority(priority), 
            self.priority_thresholds[AlertPriority.MEDIUM]
        )
        
        # Check last alert time
        last_time = self.last_alert_times.get(alert_type, 0)
        
        if (current_time - last_time) >= threshold:
            self.last_alert_times[alert_type] = current_time
            return True
        
        return False
    
    def _is_duplicate_alert(self, alert: Dict[str, Any]) -> bool:
        """Check if alert is a duplicate within deduplication window"""
        try:
            # Create alert hash
            alert_content = {
                "alert_type": alert.get("alert_type"),
                "source": alert.get("source"),
                "key_data": str(alert.get("data", {}))
            }
            
            alert_hash = hashlib.sha256(
                json.dumps(alert_content, sort_keys=True).encode()
            ).hexdigest()
            
            current_time = time.time()
            
            # Check if we've seen this alert recently
            if alert_hash in self.alert_deduplication:
                last_seen = self.alert_deduplication[alert_hash]
                if (current_time - last_seen) < self.deduplication_window:
                    return True
            
            # Update deduplication tracking
            self.alert_deduplication[alert_hash] = current_time
            
            # Cleanup old entries
            self._cleanup_deduplication_cache(current_time)
            
            return False
            
        except Exception as e:
            self.logger.error(f"Deduplication check error: {e}")
            return False
    
    def _cleanup_deduplication_cache(self, current_time: float):
        """Clean up old deduplication entries"""
        try:
            expired_hashes = [
                h for h, t in self.alert_deduplication.items() 
                if (current_time - t) > self.deduplication_window
            ]
            
            for hash_key in expired_hashes:
                del self.alert_deduplication[hash_key]
                
        except Exception as e:
            self.logger.error(f"Deduplication cleanup error: {e}")
    
    def _generate_emergency_filename(self, alert: Dict[str, Any]) -> str:
        """Generate filename for emergency alert"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        alert_type = alert.get("alert_type", "unknown").replace(" ", "_")
        return f"emergency_{alert_type}_{timestamp}.json"
    
    def _generate_alert_filename(self, alert: Dict[str, Any], priority: str) -> str:
        """Generate filename for alert"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        alert_type = alert.get("alert_type", "unknown").replace(" ", "_")
        return f"{priority}_{alert_type}_{timestamp}.json"
    
    def _save_alert_record(self, alert: Dict[str, Any]):
        """Save alert record to reaction history"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"alert_{timestamp}.json"
            filepath = Path("data/reaction_history") / filename
            
            with open(filepath, 'w') as f:
                json.dump(alert, f, indent=2)
            
            # Update alert stats
            self._update_alert_stats(alert.get("alert_type", "unknown"))
                
        except Exception as e:
            self.logger.error(f"Failed to save alert record: {e}")
    
    def _update_alert_stats(self, alert_type: str):
        """Update alert statistics"""
        try:
            stats_file = Path("data/alert_stats/alert_summary.json")
            
            if stats_file.exists():
                with open(stats_file, 'r') as f:
                    stats = json.load(f)
            else:
                stats = {"total_alerts": 0, "by_type": {}, "by_priority": {}}
            
            stats["total_alerts"] += 1
            stats["by_type"][alert_type] = stats["by_type"].get(alert_type, 0) + 1
            stats["last_updated"] = datetime.now(timezone.utc).isoformat()
            
            with open(stats_file, 'w') as f:
                json.dump(stats, f, indent=2)
                
        except Exception as e:
            self.logger.error(f"Failed to update alert stats: {e}")
    
    def _queue_immediate_notification(self, alert: Dict[str, Any]):
        """Queue alert for immediate notification (future integration)"""
        try:
            notification = {
                "type": "immediate",
                "alert": alert,
                "channels": ["webhook", "telegram", "discord"],
                "queued_at": datetime.now(timezone.utc).isoformat()
            }
            
            # Save to notification queue
            queue_file = Path("data/notification_queue") / f"immediate_{int(time.time())}.json"
            with open(queue_file, 'w') as f:
                json.dump(notification, f, indent=2)
            
            with self.lock:
                self.notification_queue.append(notification)
            
            # Future webhook integration
            if self.webhook_enabled:
                self._send_webhook_notification(notification)
            
            # Future Telegram integration
            if self.telegram_enabled:
                self._send_telegram_notification(notification)
                
        except Exception as e:
            self.logger.error(f"Immediate notification queueing error: {e}")
    
    def _queue_notification(self, alert: Dict[str, Any]):
        """Queue alert for standard notification (future integration)"""
        try:
            notification = {
                "type": "standard",
                "alert": alert,
                "channels": ["webhook", "email"],
                "queued_at": datetime.now(timezone.utc).isoformat()
            }
            
            # Save to notification queue
            queue_file = Path("data/notification_queue") / f"standard_{int(time.time())}.json"
            with open(queue_file, 'w') as f:
                json.dump(notification, f, indent=2)
            
            with self.lock:
                self.notification_queue.append(notification)
                
        except Exception as e:
            self.logger.error(f"Standard notification queueing error: {e}")
    
    def _queue_batch_notification(self, alert: Dict[str, Any]):
        """Queue alert for batch notification (future integration)"""
        try:
            notification = {
                "type": "batch",
                "alert": alert,
                "channels": ["email"],
                "queued_at": datetime.now(timezone.utc).isoformat()
            }
            
            # Save to notification queue
            queue_file = Path("data/notification_queue") / f"batch_{int(time.time())}.json"
            with open(queue_file, 'w') as f:
                json.dump(notification, f, indent=2)
                
        except Exception as e:
            self.logger.error(f"Batch notification queueing error: {e}")
    
    def _send_webhook_notification(self, notification: Dict[str, Any]):
        """Send webhook notification (future integration actual_data)"""
        try:
            # actual_data for webhook integration
            webhook_log = {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "type": "webhook",
                "notification": notification,
                "status": "placeholder_ready"
            }
            
            webhook_file = Path("data/webhook_logs") / f"webhook_{int(time.time())}.json"
            with open(webhook_file, 'w') as f:
                json.dump(webhook_log, f, indent=2)
                
            self.logger.info("Webhook notification actual_data processed")
            
        except Exception as e:
            self.logger.error(f"Webhook notification error: {e}")
    
    def _send_telegram_notification(self, notification: Dict[str, Any]):
        """Send Telegram notification (future integration actual_data)"""
        try:
            # actual_data for Telegram integration
            telegram_log = {
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "type": "telegram",
                "notification": notification,
                "status": "placeholder_ready"
            }
            
            telegram_file = Path("data/telegram_logs") / f"telegram_{int(time.time())}.json"
            with open(telegram_file, 'w') as f:
                json.dump(telegram_log, f, indent=2)
                
            self.logger.info("Telegram notification actual_data processed")
            
        except Exception as e:
            self.logger.error(f"Telegram notification error: {e}")
    
    def _flush_remaining_alerts(self):
        """Flush any remaining alerts during shutdown"""
        try:
            remaining_alerts = []
            
            while not self.alert_queue.empty():
                try:
                    priority, timestamp, alert = self.alert_queue.get_nowait()
                    if alert.get("type") != "SHUTDOWN":
                        remaining_alerts.append(alert)
                except queue.Empty:
                    break
            
            if remaining_alerts:
                flush_file = Path("data/alert_stats") / f"shutdown_flush_{int(time.time())}.json"
                with open(flush_file, 'w') as f:
                    json.dump({
                        "flushed_alerts": remaining_alerts,
                        "count": len(remaining_alerts),
                        "timestamp": datetime.now(timezone.utc).isoformat()
                    }, f, indent=2)
                
                self.logger.info(f"Flushed {len(remaining_alerts)} remaining alerts")
                
        except Exception as e:
            self.logger.error(f"Alert flush error: {e}")
    
    def get_bridge_status(self) -> Dict[str, Any]:
        """Get current bridge status and statistics"""
        with self.lock:
            return {
                "module": self.module_name,
                "version": self.version,
                "running": self.running,
                "architect_mode": self.architect_mode,
                "alert_stats": dict(self.alert_stats),
                "queue_sizes": {
                    "alert_queue": self.alert_queue.qsize(),
                    "alert_history": len(self.alert_history),
                    "emergency_alerts": len(self.emergency_alerts),
                    "notification_queue": len(self.notification_queue)
                },
                "integration_status": {
                    "webhook": self.webhook_enabled,
                    "telegram": self.telegram_enabled,
                    "email": self.email_enabled,
                    "discord": self.discord_enabled
                },
                "deduplication_cache_size": len(self.alert_deduplication),
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
    
    def create_alert(self, alert_type: str, data: Dict[str, Any], 
                     priority: AlertPriority = AlertPriority.MEDIUM, 
                     source: str = "unknown") -> None:
        """Create and queue an alert"""
        try:
            alert = {
                "alert_type": alert_type,
                "data": data,
                "source": source,
                "created_at": datetime.now(timezone.utc).isoformat()
            }
            
            self.alert_queue.put((priority.value, time.time(), alert))
            
        except Exception as e:
            self.logger.error(f"Alert creation error: {e}")
    
    # EventBus event handlers - ARCHITECT MODE COMPLIANCE
    def on_strategy_freeze_confirmed(self, event_data: Dict[str, Any]):
        """EventBus handler for StrategyFreezeConfirmed"""
        self.create_alert(
            "StrategyFreezeConfirmed",
            event_data,
            AlertPriority.HIGH,
            "ExecutionLoopResponder"
        )
    
    def on_macro_sync_completed(self, event_data: Dict[str, Any]):
        """EventBus handler for MacroSyncCompleted"""
        self.create_alert(
            "MacroSyncCompleted",
            event_data,
            AlertPriority.MEDIUM,
            "ExecutionLoopResponder"
        )
    
    def on_trade_adjustment_executed(self, event_data: Dict[str, Any]):
        """EventBus handler for TradeAdjustmentExecuted"""
        self.create_alert(
            "TradeAdjustmentExecuted",
            event_data,
            AlertPriority.MEDIUM,
            "ExecutionLoopResponder"
        )
    
    def on_emergency_alert(self, event_data: Dict[str, Any]):
        """EventBus handler for EmergencyAlert"""
        self.create_alert(
            "EmergencyAlert",
            event_data,
            AlertPriority.EMERGENCY,
            event_data.get("source", "unknown")
        )
    
    def on_kill_switch_alert(self, event_data: Dict[str, Any]):
        """EventBus handler for KillSwitchTrigger"""
        self.create_alert(
            "KillSwitchTriggered",
            event_data,
            AlertPriority.CRITICAL,
            event_data.get("source", "unknown")
        )
    
    def on_execution_deviation_alert(self, event_data: Dict[str, Any]):
        """EventBus handler for ExecutionDeviationAlert"""
        self.create_alert(
            "ExecutionDeviationAlert",
            event_data,
            AlertPriority.HIGH,
            event_data.get("source", "unknown")
        )
    
    def on_responder_termination(self, event_data: Dict[str, Any]):
        """EventBus handler for ResponderTerminationComplete"""
        self.create_alert(
            "ResponderTermination",
            event_data,
            AlertPriority.MEDIUM,
            "ExecutionLoopResponder"
        )
    
    def on_reactor_shutdown(self, event_data: Dict[str, Any]):
        """EventBus handler for ReactorShutdown"""
        self.create_alert(
            "ReactorShutdown",
            event_data,
            AlertPriority.MEDIUM,
            "SmartExecutionReactor"
        )


# Production entry point
def main():
    """Production entry point for LiveAlertBridge"""
    bridge = LiveAlertBridge()
    
    try:
        bridge.start()
        print(f"✅ GENESIS LiveAlertBridge v{bridge.version} started")
        print("🔄 Listening for StrategyFreezeConfirmed, MacroSyncCompleted, Emergency events...")
        print("📊 Logs: logs/alert_bridge/ | Data: data/emergency_alerts/, data/reaction_history/")
        print("🔗 Integration hooks: webhook, telegram, email, discord (placeholders ready)")
        
        # Keep running until interrupted
        while bridge.running:
            time.sleep(1.0)
            
    except KeyboardInterrupt:
        print("\n🛑 Shutdown requested...")
    except Exception as e:
        print(f"❌ Bridge error: {e}")
    finally:
        bridge.stop()
        print("✅ LiveAlertBridge shutdown complete")


if __name__ == "__main__":
    main()

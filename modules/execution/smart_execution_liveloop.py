# <!-- @GENESIS_MODULE_START: smart_execution_liveloop -->

from datetime import datetime\n#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ðŸ”— GENESIS EventBus Integration - Auto-injected by Orphan Recovery Engine
from datetime import datetime
import json

class SmartExecutionLiveloopEventBusIntegration:
    def detect_confluence_patterns(self, market_data: dict) -> float:
            """GENESIS Pattern Intelligence - Detect confluence patterns"""
            confluence_score = 0.0

            # Simple confluence calculation
            if market_data.get('trend_aligned', False):
                confluence_score += 0.3
            if market_data.get('support_resistance_level', False):
                confluence_score += 0.3
            if market_data.get('volume_confirmation', False):
                confluence_score += 0.2
            if market_data.get('momentum_aligned', False):
                confluence_score += 0.2

            emit_telemetry("smart_execution_liveloop", "confluence_detected", {
                "score": confluence_score,
                "timestamp": datetime.now().isoformat()
            })

            return confluence_score
    def calculate_position_size(self, risk_amount: float, stop_loss_pips: float) -> float:
            """GENESIS Risk Management - Calculate optimal position size"""
            account_balance = 100000  # Default FTMO account size
            risk_per_pip = risk_amount / stop_loss_pips if stop_loss_pips > 0 else 0
            position_size = min(risk_per_pip * 0.01, account_balance * 0.02)  # Max 2% risk

            emit_telemetry("smart_execution_liveloop", "position_calculated", {
                "risk_amount": risk_amount,
                "position_size": position_size,
                "risk_percentage": (position_size / account_balance) * 100
            })

            return position_size
    """EventBus integration for smart_execution_liveloop"""
    
    def __init__(self):
        self.module_id = "smart_execution_liveloop"
        self.event_routes = []
        
    def emit_event(self, event_type, data):
        """Emit event to EventBus"""
        event = {
            "timestamp": datetime.now().isoformat(),
            "module": self.module_id,
            "event_type": event_type,
            "data": data
        }
        print(f"ðŸ”— EVENTBUS EMIT: {event}")
        
    def emit_telemetry(self, metric_name, value):
        """Emit telemetry data"""
        telemetry = {
            "timestamp": datetime.now().isoformat(),
            "module": self.module_id,
            "metric": metric_name,
            "value": value
        }
        print(f"ðŸ“Š TELEMETRY: {telemetry}")

# Auto-instantiate EventBus integration
smart_execution_liveloop_eventbus = SmartExecutionLiveloopEventBusIntegration()

"""
GENESIS AI TRADING BOT SYSTEM - PHASE 17
SmartExecutionLiveLoop - Self-correcting real-time execution loop with dynamic telemetry
ARCHITECT MODE: v2.7
"""
import os
import json
import logging
import datetime
import statistics
import time
import numpy as np
import pandas as pd
from pathlib import Path
from collections import defaultdict, deque
from threading import Thread, Lock, Event
import sys

# Add parent directory to path to import event_bus
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from event_bus import get_event_bus, register_route

class SmartExecutionLiveLoop:
    def detect_confluence_patterns(self, market_data: dict) -> float:
            """GENESIS Pattern Intelligence - Detect confluence patterns"""
            confluence_score = 0.0

            # Simple confluence calculation
            if market_data.get('trend_aligned', False):
                confluence_score += 0.3
            if market_data.get('support_resistance_level', False):
                confluence_score += 0.3
            if market_data.get('volume_confirmation', False):
                confluence_score += 0.2
            if market_data.get('momentum_aligned', False):
                confluence_score += 0.2

            emit_telemetry("smart_execution_liveloop", "confluence_detected", {
                "score": confluence_score,
                "timestamp": datetime.now().isoformat()
            })

            return confluence_score
    def calculate_position_size(self, risk_amount: float, stop_loss_pips: float) -> float:
            """GENESIS Risk Management - Calculate optimal position size"""
            account_balance = 100000  # Default FTMO account size
            risk_per_pip = risk_amount / stop_loss_pips if stop_loss_pips > 0 else 0
            position_size = min(risk_per_pip * 0.01, account_balance * 0.02)  # Max 2% risk

            emit_telemetry("smart_execution_liveloop", "position_calculated", {
                "risk_amount": risk_amount,
                "position_size": position_size,
                "risk_percentage": (position_size / account_balance) * 100
            })

            return position_size
    """
    Self-correcting real-time execution loop that monitors trade execution quality,
    provides adaptive logging, and enforces kill-switch protocols when needed.
    
    This service is responsible for:
    - Monitoring live trade execution quality vs backtest expectations
    - Generating alerts on execution deviation (latency, slippage)
    - Requesting recalibration when performance deteriorates
    - Enforcing kill-switch if critical thresholds are breached
    - Logging all activity in structured JSONL format
    - Generating telemetry for real-time monitoring
    
    ARCHITECT MODE COMPLIANCE:
    - Event-driven only - ALL data through EventBus
    - Real MT5 data only - no real or execute data
    - Telemetry and compliance hooks throughout
    - Structured logging for institutional compliance
    """
    
    # Configuration parameters
    SEL_CONFIG = {
        "max_allowed_slippage": 0.9,       # Maximum allowed slippage in pips
        "latency_threshold_ms": 450,       # Latency threshold in milliseconds
        "kill_switch_alerts": 3,           # Number of alerts in timeframe to trigger kill switch
        "kill_switch_timeframe_min": 5,    # Timeframe in minutes for alert counting
        "max_drawdown_pct": 12.5,          # Maximum drawdown percentage before kill switch
        "log_sync_interval_min": 5,        # Log sync interval in minutes
        "health_metric_interval_hour": 1,  # Health metric emission interval in hours
        "histogram_bins": 20               # Number of bins for latency histogram
    }
    
    def __init__(self):
        """Initialize the SmartExecutionLiveLoop with all required components"""        
        # Configure directories
        self._setup_directories()
        
        # Configure logging
        self._setup_logging()
        
        # Setup EventBus connection using singleton pattern (ARCHITECT MODE COMPLIANCE)
        self.event_bus = get_event_bus()        
        
        # Initialize data storage
        self.live_trades = []
        self.backtest_results = {}
        self.trade_journal_entries = []
        self.execution_logs = []
        
        # Initialize alert tracking for kill switch logic
        self.recent_alerts = deque(maxlen=100)  # Store recent alerts with timestamps
        self.alert_lock = Lock()                # Thread-safety for alert tracking
        
        # Initialize telemetry metrics
        self.metrics = {
            "latency_histogram": defaultdict(int),  # Execution latency distribution
            "execution_slippage": [],                # Slippage values
            "kill_trigger_count": 0,                 # Number of kill switch triggers
            "recalibration_requests": 0,             # Number of recalibration requests
            "trades_monitored": 0,                   # Total trades monitored
            "alert_count_by_type": defaultdict(int), # Alert counts by type
            "last_backtest_metrics": {},             # Last received backtest metrics
            "running_drawdown": 0.0,                 # Current running drawdown
        }
        
        # Thread control
        self.running = Event()
        self.running.set()
        
        # Start service threads
        self.log_sync_thread = self._start_log_sync_thread()
        self.health_metric_thread = self._start_health_metric_thread()
        
        # Register EventBus subscribers
        self._subscribe_to_events()
        
        # Register EventBus routes for compliance tracking (ARCHITECT MODE)
        self._register_compliance_routes()
        
        self.logger.info(json.dumps({
            "event": "initialization",
            "service": "SmartExecutionLiveLoop",
            "timestamp": datetime.datetime.now().isoformat(),
            "status": "started"
        }))
        
        # Emit telemetry that module is active
        self._emit_telemetry("initialization", "SmartExecutionLiveLoop service activated successfully")

    
        # GENESIS Phase 91 Telemetry Injection
        if hasattr(self, 'event_bus') and self.event_bus:
            self.event_bus.emit("telemetry", {
                "module": __name__,
                "status": "running",
                "timestamp": datetime.now().isoformat(),
                "phase": "91_telemetry_enforcement"
            })
        def _setup_directories(self):
        """Ensure all required directories exist"""
        # Log directory
        self.log_dir = Path("logs/liveloop")
        self.log_dir.mkdir(exist_ok=True, parents=True)
        
        # Telemetry directory
        self.telemetry_dir = Path("data/liveloop_stats")
        self.telemetry_dir.mkdir(exist_ok=True, parents=True)
    
    def _setup_logging(self):
        """Configure structured JSONL logging for the module"""
        # Configure logger
        self.logger = logging.getLogger("SmartExecutionLiveLoop")
        self.logger.setLevel(logging.INFO)
        
        # Remove any existing handlers
        for handler in self.logger.handlers[:]:
            self.logger.removeHandler(handler)
        
        # Create handlers
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d")
        log_file = self.log_dir / f"liveloop_{timestamp}.jsonl"
        file_handler = logging.FileHandler(log_file)
        
        # Create formatter for JSONL structured logging
        formatter = logging.Formatter('{"timestamp": "%(asctime)s", "level": "%(levelname)s", "message": %(message)s}')
        file_handler.setFormatter(formatter)
        
        # Add handlers
        self.logger.addHandler(file_handler)
    
    def _subscribe_to_events(self):
        """Subscribe to all required events from the EventBus"""
        self.event_bus.subscribe("LiveTradeExecuted", self.handle_live_trade, "SmartExecutionLiveLoop")
        self.event_bus.subscribe("TradeJournalEntry", self.handle_journal_entry, "SmartExecutionLiveLoop")
        self.event_bus.subscribe("ExecutionLog", self.handle_execution_log, "SmartExecutionLiveLoop")
        self.event_bus.subscribe("BacktestResults", self.handle_backtest_results, "SmartExecutionLiveLoop")
        self.event_bus.subscribe("KillSwitchTrigger", self.handle_kill_switch, "SmartExecutionLiveLoop")
        
        self.logger.info(json.dumps({
            "event": "event_subscriptions_registered",
            "topics": ["LiveTradeExecuted", "TradeJournalEntry", "ExecutionLog", "BacktestResults", "KillSwitchTrigger"]
        }))
    
    def _register_compliance_routes(self):
        """Register all EventBus routes for ARCHITECT MODE compliance tracking"""
        # Register input routes (what this module consumes)
        register_route("LiveTradeExecuted", "ExecutionEngine", "SmartExecutionLiveLoop")
        register_route("TradeJournalEntry", "TradeJournalEngine", "SmartExecutionLiveLoop")
        register_route("ExecutionLog", "ExecutionEngine", "SmartExecutionLiveLoop")
        register_route("BacktestResults", "BacktestEngine", "SmartExecutionLiveLoop")
        register_route("KillSwitchTrigger", "RiskEngine", "SmartExecutionLiveLoop")
        
        # For testing
        register_route("LiveTradeExecuted", "TestSmartExecutionMonitor", "SmartExecutionLiveLoop")
        register_route("TradeJournalEntry", "TestSmartExecutionMonitor", "SmartExecutionLiveLoop")
        register_route("ExecutionLog", "TestSmartExecutionMonitor", "SmartExecutionLiveLoop")
        register_route("BacktestResults", "TestSmartExecutionMonitor", "SmartExecutionLiveLoop")
        
        # Register output routes (what this module produces)
        register_route("ExecutionDeviationAlert", "SmartExecutionLiveLoop", "RiskEngine")
        register_route("RecalibrationRequest", "SmartExecutionLiveLoop", "StrategyRecommenderEngine")
        register_route("SmartLogSync", "SmartExecutionLiveLoop", "DashboardEngine")
        register_route("KillSwitchTrigger", "SmartExecutionLiveLoop", "RiskEngine")
        register_route("LoopHealthMetric", "SmartExecutionLiveLoop", "TelemetryCollector")
        
        self.logger.info(json.dumps({
            "event": "eventbus_routes_registered",
            "status": "ARCHITECT_MODE_COMPLIANT"
        }))
    
    def handle_live_trade(self, event):
        """
        Handle LiveTradeExecuted events
        Monitors execution quality (latency, slippage) and triggers alerts
        
        Expected event schema:
        {
            "trade_id": "str",
            "strategy_id": "str",
            "symbol": "str",
            "direction": "BUY|SELL",
            "execution_time_ms": float,
            "slippage_pips": float,
            "execution_price": float,
            "target_price": float,
            "volume": float,
            "timestamp": "ISO-datetime"
        }
        """
        try:
            trade_data = event.get("data", {})
            trade_id = trade_data.get("trade_id", "unknown")
            
            self.logger.info(json.dumps({
                "event": "live_trade_received", 
                "trade_id": trade_id
            }))
            
            # Store trade for analysis
            self.live_trades.append(trade_data)
            self.metrics["trades_monitored"] += 1
            
            # Check for execution deviations
            self._check_execution_deviations(trade_data)
            
            # Update latency histogram
            latency_ms = trade_data.get("execution_time_ms", 0)
            if latency_ms > 0:  # Avoid recording zero latency which might be invalid data
                bin_key = (latency_ms // 50) * 50  # Group in 50ms bins (0-50, 50-100, etc.)
                self.metrics["latency_histogram"][bin_key] += 1
            
            # Update slippage metrics
            slippage = trade_data.get("slippage_pips", 0)
            if abs(slippage) > 0:  # Avoid recording zero slippage which might be invalid data
                self.metrics["execution_slippage"].append(slippage)
            
            # Write telemetry data
            self._write_telemetry_data()
            
        except Exception as e:
            self.handle_errors(f"Error processing live trade: {str(e)}")
    
    def handle_journal_entry(self, event):
        """
        Handle TradeJournalEntry events
        Tracks trade outcomes, drawdown, and performance patterns
        
        Expected event schema:
        {
            "entry_id": "str",
            "trade_id": "str",
            "strategy_id": "str",
            "outcome": "WIN|LOSS|BREAKEVEN",
            "profit_loss": float,
            "running_balance": float,
            "timestamp": "ISO-datetime"
        }
        """
        try:
            journal_data = event.get("data", {})
            entry_id = journal_data.get("entry_id", "unknown")
            
            self.logger.info(json.dumps({
                "event": "journal_entry_received", 
                "entry_id": entry_id
            }))
            
            # Store journal entry for analysis
            self.trade_journal_entries.append(journal_data)
            
            # Update running drawdown calculation
            self._update_drawdown_metrics(journal_data)
            
            # Check if we need to emit health metrics based on journal entries
            if len(self.trade_journal_entries) % 5 == 0:  # Every 5 journal entries
                self._emit_health_metric()
            
        except Exception as e:
            self.handle_errors(f"Error processing journal entry: {str(e)}")
    
    def handle_execution_log(self, event):
        """
        Handle ExecutionLog events
        Monitors execution details and system performance
        
        Expected event schema:
        {
            "log_id": "str",
            "execution_phase": "ORDER_CREATED|ORDER_SENT|ORDER_FILLED|...",
            "latency_ms": float,
            "success": bool,
            "message": "str",
            "trade_id": "str",
            "timestamp": "ISO-datetime"
        }
        """
        try:
            log_data = event.get("data", {})
            log_id = log_data.get("log_id", "unknown")
            
            self.logger.info(json.dumps({
                "event": "execution_log_received", 
                "log_id": log_id,
                "execution_phase": log_data.get("execution_phase", "unknown")
            }))
            
            # Store execution log for analysis
            self.execution_logs.append(log_data)
            
            # Check for system latency issues
            latency = log_data.get("latency_ms")
            if latency and latency > self.SEL_CONFIG["latency_threshold_ms"]:
                self._emit_deviation_alert("execution_latency", {
                    "latency_ms": latency,
                    "threshold_ms": self.SEL_CONFIG["latency_threshold_ms"],
                    "execution_phase": log_data.get("execution_phase"),
                    "trade_id": log_data.get("trade_id")
                })
            
        except Exception as e:
            self.handle_errors(f"Error processing execution log: {str(e)}")
    
    def handle_backtest_results(self, event):
        """
        Handle BacktestResults events
        Updates baseline expectations for monitoring
        
        Expected event schema:
        {
            "backtest_id": "str",
            "strategy_id": "str",
            "win_rate": float,
            "avg_slippage": float,
            "expected_latency_ms": float,
            "drawdown_pct": float,
            "sharpe_ratio": float,
            "timestamp": "ISO-datetime"
        }
        """
        try:
            backself.event_bus.request('data:live_feed') = event.get("data", {})
            backtest_id = backself.event_bus.request('data:live_feed').get("backtest_id", "unknown")
            strategy_id = backself.event_bus.request('data:live_feed').get("strategy_id", "unknown")
            
            self.logger.info(json.dumps({
                "event": "backtest_results_received", 
                "backtest_id": backtest_id,
                "strategy_id": strategy_id
            }))
            
            # Store backtest results for comparison
            self.backtest_results[strategy_id] = backself.event_bus.request('data:live_feed')
            
            # Store latest backtest metrics for telemetry
            if backself.event_bus.request('data:live_feed'):
                self.metrics["last_backtest_metrics"] = {
                    "strategy_id": strategy_id,
                    "win_rate": backself.event_bus.request('data:live_feed').get("win_rate", 0),
                    "drawdown_pct": backself.event_bus.request('data:live_feed').get("drawdown_pct", 0),
                    "avg_slippage": backself.event_bus.request('data:live_feed').get("avg_slippage", 0),
                    "expected_latency_ms": backself.event_bus.request('data:live_feed').get("expected_latency_ms", 0),
                }
            
            # Compare live execution to backtest for this strategy
            self._compare_live_to_backtest(strategy_id)
            
        except Exception as e:
            self.handle_errors(f"Error processing backtest results: {str(e)}")
    
    def handle_kill_switch(self, event):
        """
        Handle KillSwitchTrigger events
        Tracks kill switch activations for system monitoring
        
        Expected event schema:
        {
            "trigger_id": "str",
            "source": "str",
            "reason": "str",
            "strategy_id": "str",
            "timestamp": "ISO-datetime"
        }
        """
        try:
            kill_switch_data = event.get("data", {})
            source = kill_switch_data.get("source", "unknown")
            
            # Only track if we didn't generate this event (avoid loops)
            if source != "SmartExecutionLiveLoop":
                self.logger.warning(json.dumps({
                    "event": "kill_switch_received", 
                    "source": source,
                    "reason": kill_switch_data.get("reason", "unknown")
                }))
                
                # Update kill switch counter
                self.metrics["kill_trigger_count"] += 1
                
                # Force write telemetry when kill switch is activated
                self._write_telemetry_data(force=True)
            
        except Exception as e:
            self.handle_errors(f"Error processing kill switch: {str(e)}")
    
    def _check_execution_deviations(self, trade_data):
        """Check for deviations in trade execution quality"""
        strategy_id = trade_data.get("strategy_id", "unknown")
        trade_id = trade_data.get("trade_id", "unknown")
        
        # Check slippage deviation
        slippage = trade_data.get("slippage_pips", 0)
        if abs(slippage) > self.SEL_CONFIG["max_allowed_slippage"]:
            self.logger.warning(json.dumps({
                "event": "high_slippage_detected",
                "trade_id": trade_id,
                "strategy_id": strategy_id,
                "slippage": slippage,
                "threshold": self.SEL_CONFIG["max_allowed_slippage"]
            }))
            
            # Emit deviation alert for high slippage
            self._emit_deviation_alert("high_slippage", {
                "trade_id": trade_id,
                "strategy_id": strategy_id,
                "slippage": slippage,
                "threshold": self.SEL_CONFIG["max_allowed_slippage"],
                "symbol": trade_data.get("symbol", "unknown"),
                "direction": trade_data.get("direction", "unknown")
            })
        
        # Check execution latency
        latency = trade_data.get("execution_time_ms", 0)
        if latency > self.SEL_CONFIG["latency_threshold_ms"]:
            self.logger.warning(json.dumps({
                "event": "high_latency_detected",
                "trade_id": trade_id,
                "strategy_id": strategy_id,
                "latency_ms": latency,
                "threshold_ms": self.SEL_CONFIG["latency_threshold_ms"]
            }))
            
            # Emit deviation alert for high latency
            self._emit_deviation_alert("high_latency", {
                "trade_id": trade_id,
                "strategy_id": strategy_id,
                "latency_ms": latency,
                "threshold_ms": self.SEL_CONFIG["latency_threshold_ms"],
                "symbol": trade_data.get("symbol", "unknown")
            })
    
    def _compare_live_to_backtest(self, strategy_id):
        """Compare live execution metrics to backtest expectations for a strategy"""
        if strategy_id not in self.backtest_results:
            return
        
        # Get all trades for this strategy
        strategy_trades = [t for t in self.live_trades if t.get("strategy_id") == strategy_id]
        
        # Need minimum number of trades before meaningful comparison
        if len(strategy_trades) < 5:
            return
        
        backtest = self.backtest_results[strategy_id]
        
        # Calculate live metrics
        avg_slippage = statistics.mean([t.get("slippage_pips", 0) for t in strategy_trades])
        avg_latency = statistics.mean([t.get("execution_time_ms", 0) for t in strategy_trades])
        
        # Compare with backtest expectations
        backtest_slippage = backtest.get("avg_slippage", 0)
        backtest_latency = backtest.get("expected_latency_ms", 0)
        
        # Check for significant degradation
        slippage_degradation = avg_slippage > (backtest_slippage * 1.5)
        latency_degradation = avg_latency > (backtest_latency * 1.5)
        
        if slippage_degradation or latency_degradation:
            self.logger.warning(json.dumps({
                "event": "performance_degradation_detected",
                "strategy_id": strategy_id,
                "slippage_degradation": slippage_degradation,
                "latency_degradation": latency_degradation,
                "live_avg_slippage": avg_slippage,
                "backtest_avg_slippage": backtest_slippage,
                "live_avg_latency": avg_latency,
                "backtest_avg_latency": backtest_latency
            }))
            
            # Request recalibration
            details = {
                "strategy_id": strategy_id,
                "metrics": {
                    "slippage": {
                        "live": avg_slippage,
                        "backtest": backtest_slippage,
                        "degradation_pct": (avg_slippage / backtest_slippage - 1) * 100 if backtest_slippage > 0 else 0
                    },
                    "latency": {
                        "live": avg_latency,
                        "backtest": backtest_latency,
                        "degradation_pct": (avg_latency / backtest_latency - 1) * 100 if backtest_latency > 0 else 0
                    }
                }
            }
            self._emit_recalibration_request(strategy_id, details)
    
    def _update_drawdown_metrics(self, journal_entry):
        """Update and monitor drawdown metrics from journal entries"""
        running_balance = journal_entry.get("running_balance")
        profit_loss = journal_entry.get("profit_loss")
        
        if running_balance is not None:
            # Get peak balance so far
            if not hasattr(self, "peak_balance"):
                self.peak_balance = running_balance
            elif running_balance > self.peak_balance:
                self.peak_balance = running_balance
            
            # Calculate drawdown percentage
            if self.peak_balance > 0:
                current_drawdown = (self.peak_balance - running_balance) / self.peak_balance * 100
                self.metrics["running_drawdown"] = current_drawdown
                
                # Check if drawdown exceeds threshold
                if current_drawdown > self.SEL_CONFIG["max_drawdown_pct"]:
                    self.logger.critical(json.dumps({
                        "event": "drawdown_threshold_exceeded",
                        "current_drawdown_pct": current_drawdown,
                        "threshold_pct": self.SEL_CONFIG["max_drawdown_pct"],
                        "peak_balance": self.peak_balance,
                        "current_balance": running_balance
                    }))
                    
                    # Trigger kill switch for excessive drawdown
                    self._emit_kill_switch("excessive_drawdown", {
                        "current_drawdown_pct": current_drawdown,
                        "threshold_pct": self.SEL_CONFIG["max_drawdown_pct"],
                        "peak_balance": self.peak_balance,
                        "current_balance": running_balance
                    })
    
    def _emit_deviation_alert(self, alert_type, details):
        """Emit an execution deviation alert"""
        current_time = datetime.datetime.now()
        alert_data = {
            "timestamp": current_time.isoformat(),
            "alert_type": alert_type,
            "details": details,
            "severity": "high" if alert_type in ["high_slippage", "excessive_drawdown"] else "medium"
        }
        
        # Increment alert type counter
        self.metrics["alert_count_by_type"][alert_type] += 1
        
        # Record this alert with timestamp for kill switch monitoring
        with self.alert_lock:
            self.recent_alerts.append({
                "timestamp": current_time,
                "type": alert_type,
                "details": details
            })
            
            # Check if we need to trigger kill switch based on alert frequency
            self._check_alert_frequency_kill_switch()
        
        # Emit the alert via EventBus
        self.event_bus.emit_event("ExecutionDeviationAlert", alert_data, "SmartExecutionLiveLoop")
        
        # Sync logs after alert
        self._emit_log_sync(alert_type, details)
        
        self.logger.info(json.dumps({
            "event": "deviation_alert_emitted",
            "alert_type": alert_type
        }))
    
    def _emit_recalibration_request(self, strategy_id, details):
        """Emit a recalibration request"""
        recal_data = {
            "timestamp": datetime.datetime.now().isoformat(),
            "strategy_id": strategy_id,
            "reason": "performance_degradation",
            "details": details
        }
        
        # Increment recalibration request counter
        self.metrics["recalibration_requests"] += 1
        
        # Emit the recalibration request via EventBus
        self.event_bus.emit_event("RecalibrationRequest", recal_data, "SmartExecutionLiveLoop")
        
        # Sync logs after recalibration request
        self._emit_log_sync("recalibration_requested", {
            "strategy_id": strategy_id,
            "reason": "performance_degradation"
        })
        
        self.logger.info(json.dumps({
            "event": "recalibration_requested",
            "strategy_id": strategy_id
        }))
    
    def _emit_kill_switch(self, reason, details):
        """Emit a kill switch trigger"""
        kill_switch_data = {
            "timestamp": datetime.datetime.now().isoformat(),
            "reason": reason,
            "details": details,
            "source": "SmartExecutionLiveLoop"
        }
        
        # Increment kill switch counter
        self.metrics["kill_trigger_count"] += 1
        
        # Emit the kill switch trigger via EventBus
        self.event_bus.emit_event("KillSwitchTrigger", kill_switch_data, "SmartExecutionLiveLoop")
        
        # Sync logs after kill switch
        self._emit_log_sync("kill_switch_triggered", {
            "reason": reason,
            "details": details
        })
        
        self.logger.critical(json.dumps({
            "event": "kill_switch_triggered",
            "reason": reason
        }))
        
        # Force write telemetry when kill switch is triggered
        self._write_telemetry_data(force=True)
    
    def _emit_log_sync(self, event_type, details):
        """Emit a log sync event for dashboard updating"""
        log_data = {
            "timestamp": datetime.datetime.now().isoformat(),
            "event_type": event_type,
            "details": details,
            "source": "SmartExecutionLiveLoop"
        }
        
        self.event_bus.emit_event("SmartLogSync", log_data, "SmartExecutionLiveLoop")
    
    def _emit_health_metric(self):
        """Emit health metrics for system monitoring"""
        # Calculate metrics
        avg_latency = statistics.mean([t.get("execution_time_ms", 0) for t in self.live_trades[-20:]]) if self.live_trades else 0
        avg_slippage = statistics.mean([t.get("slippage_pips", 0) for t in self.live_trades[-20:]]) if self.live_trades else 0
        active_strategies = len(set([t.get("strategy_id") for t in self.live_trades[-50:]]))
        
        # Create health metric data
        health_data = {
            "timestamp": datetime.datetime.now().isoformat(),
            "avg_latency_ms": avg_latency,
            "avg_slippage_pips": avg_slippage,
            "active_strategies": active_strategies,
            "trades_monitored": self.metrics["trades_monitored"],
            "alerts_generated": sum(self.metrics["alert_count_by_type"].values()),
            "recalibrations_requested": self.metrics["recalibration_requests"],
            "kill_switch_triggers": self.metrics["kill_trigger_count"],
            "running_drawdown_pct": self.metrics["running_drawdown"]
        }
        
        # Emit via EventBus
        self.event_bus.emit_event("LoopHealthMetric", health_data, "SmartExecutionLiveLoop")
        
        self.logger.info(json.dumps({
            "event": "health_metric_emitted",
            "metrics": {
                "avg_latency_ms": avg_latency,
                "avg_slippage_pips": avg_slippage,
                "active_strategies": active_strategies,
                "alerts_count": sum(self.metrics["alert_count_by_type"].values())
            }
        }))
    
    def _start_log_sync_thread(self):
        """Start a thread to periodically sync logs"""
        def sync_logs():
            while self.running.is_set():
                try:
                    # Emit a log sync event
                    summary = {
                        "trades_count": len(self.live_trades),
                        "alerts_count": sum(self.metrics["alert_count_by_type"].values()),
                        "recalibration_requests": self.metrics["recalibration_requests"],
                        "kill_switch_triggers": self.metrics["kill_trigger_count"]
                    }
                    
                    self._emit_log_sync("periodic_sync", summary)
                    
                    # Sleep for the configured interval
                    time.sleep(self.SEL_CONFIG["log_sync_interval_min"] * 60)
                except Exception as e:
                    self.handle_errors(f"Error in log sync thread: {str(e)}")
                    time.sleep(60)  # Sleep for a minute before trying again
        
        thread = Thread(target=sync_logs, daemon=True)
        thread.start()
        return thread
    
    def _start_health_metric_thread(self):
        """Start a thread to periodically emit health metrics"""
        def emit_health():
            while self.running.is_set():
                try:
                    # Emit health metrics
                    self._emit_health_metric()
                    
                    # Sleep for the configured interval
                    time.sleep(self.SEL_CONFIG["health_metric_interval_hour"] * 3600)
                except Exception as e:
                    self.handle_errors(f"Error in health metric thread: {str(e)}")
                    time.sleep(600)  # Sleep for 10 minutes before trying again
        
        thread = Thread(target=emit_health, daemon=True)
        thread.start()
        return thread
    
    def _check_alert_frequency_kill_switch(self):
        """Check if we need to trigger kill switch based on alert frequency"""
        # Define timeframe for alert counting
        time_threshold = datetime.datetime.now() - datetime.timedelta(minutes=self.SEL_CONFIG["kill_switch_timeframe_min"])
        
        # Count alerts within timeframe
        recent_alert_count = sum(1 for alert in self.recent_alerts if alert["timestamp"] >= time_threshold)
        
        # Trigger kill switch if too many alerts in short timeframe
        if recent_alert_count >= self.SEL_CONFIG["kill_switch_alerts"]:
            self.logger.critical(json.dumps({
                "event": "frequent_alerts_kill_switch",
                "alert_count": recent_alert_count,
                "timeframe_minutes": self.SEL_CONFIG["kill_switch_timeframe_min"],
                "threshold": self.SEL_CONFIG["kill_switch_alerts"]
            }))
            
            # Get alert types in the period
            alert_types = [alert["type"] for alert in self.recent_alerts if alert["timestamp"] >= time_threshold]
            
            # Emit kill switch
            self._emit_kill_switch("frequent_alerts", {
                "alert_count": recent_alert_count,
                "timeframe_minutes": self.SEL_CONFIG["kill_switch_timeframe_min"],
                "threshold": self.SEL_CONFIG["kill_switch_alerts"],
                "alert_types": alert_types
            })
    
    def _write_telemetry_data(self, force=False):
        """Write telemetry data to disk"""
        # Only write periodically or when forced
        if not force and self.metrics["trades_monitored"] % 10 != 0:
            return
            
        try:
            # Create timestamp
            timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            
            # Format latency histogram for serialization (convert defaultdict to dict)
            latency_histogram = dict(self.metrics["latency_histogram"])
            
            # Prepare telemetry data
            telemetry = {
                "timestamp": datetime.datetime.now().isoformat(),
                "latency_histogram": latency_histogram,
                "execution_slippage": self.metrics["execution_slippage"][-100:] if self.metrics["execution_slippage"] else [],
                "kill_trigger_count": self.metrics["kill_trigger_count"],
                "recalibration_requests": self.metrics["recalibration_requests"],
                "trades_monitored": self.metrics["trades_monitored"],
                "alert_count_by_type": dict(self.metrics["alert_count_by_type"]),
                "running_drawdown": self.metrics["running_drawdown"]
            }
            
            # Write to file
            telemetry_file = self.telemetry_dir / f"liveloop_stats_{timestamp}.json"
            with open(telemetry_file, "w") as f:
                json.dump(telemetry, f, indent=2)
                
            self.logger.info(json.dumps({
                "event": "telemetry_data_written",
                "file": str(telemetry_file)
            }))
            
        except Exception as e:
            self.handle_errors(f"Error writing telemetry: {str(e)}")
    
    def _emit_telemetry(self, action, message):
        """Emit telemetry data via EventBus"""
        telemetry_data = {
            "timestamp": datetime.datetime.now().isoformat(),
            "module": "SmartExecutionLiveLoop",
            "action": action,
            "message": message,
            "metrics": {
                "trades_monitored": self.metrics["trades_monitored"],
                "alert_count": sum(self.metrics["alert_count_by_type"].values()),
                "kill_trigger_count": self.metrics["kill_trigger_count"],
                "recalibration_requests": self.metrics["recalibration_requests"],
                "running_drawdown": self.metrics["running_drawdown"]
            }
        }
        
        self.event_bus.emit_event("ModuleTelemetry", telemetry_data, "SmartExecutionLiveLoop")
    
    def handle_errors(self, error_msg, error_type="general"):
        """Handle and log errors"""
        self.logger.error(json.dumps({
            "event": "module_error", 
            "error_type": error_type,
            "error_message": error_msg
        }))
        
        # Emit error via EventBus
        error_data = {
            "timestamp": datetime.datetime.now().isoformat(),
            "module": "SmartExecutionLiveLoop",
            "error_type": error_type,
            "message": error_msg
        }
        
        self.event_bus.emit_event("ModuleError", error_data, "SmartExecutionLiveLoop")
      def stop(self):
        """Stop all threads and clean up"""
        self.running.clear()
        
        # Log shutdown
        self.logger.info(json.dumps({
            "event": "service_stopped",
            "timestamp": datetime.datetime.now().isoformat(),
            "reason": "manual_shutdown"
        }))
        
        # Final telemetry update
        self._write_telemetry_data(force=True)
        
        # Wait for threads to terminate
        if hasattr(self, 'log_sync_thread') and self.log_sync_thread.is_alive():
            self.log_sync_thread.join(timeout=2)
            
        if hasattr(self, 'health_metric_thread') and self.health_metric_thread.is_alive():
            self.health_metric_thread.join(timeout=2)

# Initialize if run directly
if __name__ == "__main__":
    import signal
    
    sel = None
    
    def signal_handler(sig, frame):
        if sel is not None:
            sel.stop()
        sys.exit(0)
    
    # Register signal handlers
    signal.signal(signal.SIGINT, signal_handler)  # For Ctrl+C
    signal.signal(signal.SIGTERM, signal_handler)  # For system termination
    
    try:
        sel = SmartExecutionLiveLoop()
        print("SmartExecutionLiveLoop is running. Press Ctrl+C to stop.")
        
        # Keep the process running, but in a way that can be interrupted
        while sel.running.is_set():
            time.sleep(1)
            
    except Exception as e:
        logging.error(f"Error in SmartExecutionLiveLoop: {str(e)}")
        if sel is not None:
            sel.stop()
        sys.exit(1)

    def log_state(self):
        """Phase 91 Telemetry Enforcer - Log current module state"""
        state_data = {
            "module": __name__,
            "timestamp": datetime.now().isoformat(),
            "status": "active",
            "phase": "91_telemetry_enforcement"
        }
        if hasattr(self, 'event_bus') and self.event_bus:
            self.event_bus.emit("telemetry", state_data)
        return state_data
        

# <!-- @GENESIS_MODULE_END: smart_execution_liveloop -->

# üìä GENESIS Telemetry Integration - Auto-injected by Complete Intelligent Wiring Engine
try:
    from core.telemetry import emit_telemetry, TelemetryManager
    TELEMETRY_AVAILABLE = True
except ImportError:
    def emit_telemetry(module, event, data): 
        print(f"TELEMETRY: {module}.{event} - {data}")
    class TelemetryManager:
        def emergency_stop(self, reason: str = "Manual trigger") -> bool:
                """GENESIS Emergency Kill Switch"""
                try:
                    # Emit emergency event
                    if hasattr(self, 'event_bus') and self.event_bus:
                        emit_event("emergency_stop", {
                            "module": "test_phase57_58_integration_recovered_2",
                            "reason": reason,
                            "timestamp": datetime.now().isoformat()
                        })

                    # Log telemetry
                    self.emit_module_telemetry("emergency_stop", {
                        "reason": reason,
                        "timestamp": datetime.now().isoformat()
                    })

                    # Set emergency state
                    if hasattr(self, '_emergency_stop_active'):
                        self._emergency_stop_active = True

                    return True
                except Exception as e:
                    print(f"Emergency stop error in test_phase57_58_integration_recovered_2: {e}")
                    return False
        def validate_ftmo_compliance(self, trade_data: dict) -> bool:
                """GENESIS FTMO Compliance Validator"""
                # Daily drawdown check (5%)
                daily_loss = trade_data.get('daily_loss_pct', 0)
                if daily_loss > 5.0:
                    self.emit_module_telemetry("ftmo_violation", {
                        "type": "daily_drawdown", 
                        "value": daily_loss,
                        "threshold": 5.0
                    })
                    return False

                # Maximum drawdown check (10%)
                max_drawdown = trade_data.get('max_drawdown_pct', 0)
                if max_drawdown > 10.0:
                    self.emit_module_telemetry("ftmo_violation", {
                        "type": "max_drawdown", 
                        "value": max_drawdown,
                        "threshold": 10.0
                    })
                    return False

                # Risk per trade check (2%)
                risk_pct = trade_data.get('risk_percent', 0)
                if risk_pct > 2.0:
                    self.emit_module_telemetry("ftmo_violation", {
                        "type": "risk_exceeded", 
                        "value": risk_pct,
                        "threshold": 2.0
                    })
                    return False

                return True
        def emit_module_telemetry(self, event: str, data: dict = None):
                """GENESIS Module Telemetry Hook"""
                telemetry_data = {
                    "timestamp": datetime.now().isoformat(),
                    "module": "test_phase57_58_integration_recovered_2",
                    "event": event,
                    "data": data or {}
                }
                try:
                    emit_telemetry("test_phase57_58_integration_recovered_2", event, telemetry_data)
                except Exception as e:
                    print(f"Telemetry error in test_phase57_58_integration_recovered_2: {e}")
        def emit(self, event, data): pass
    TELEMETRY_AVAILABLE = False



# üîó GENESIS EventBus Integration - Auto-injected by Complete Intelligent Wiring Engine
try:
    from core.hardened_event_bus import get_event_bus, emit_event, register_route
    EVENTBUS_AVAILABLE = True
except ImportError:
    # Fallback implementation
    def get_event_bus(): return None
    def emit_event(event, data): print(f"EVENT: {event} - {data}")
    def register_route(route, producer, consumer): pass
    EVENTBUS_AVAILABLE = False


#!/usr/bin/env python3
"""
GENESIS Phase 57-58 Integration Validation Test
Tests ML Retraining Loop and Pattern Learning Engine integration
"""

import os
import json
import sys
import time
import logging
from datetime import datetime
from threading import Thread

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_file_structure():
    """Test that all required Phase 57-58 files exist"""
    logger.info("üîç Testing file structure...")
    
    required_files = [
        "ml_retraining_loop_phase57.py",
        "pattern_learning_engine_phase58.py",
        "event_bus.json",
        "telemetry.json",
        "build_status.json"
    ]
    
    missing_files = []
    for file in required_files:
        if not os.path.exists(file):
            missing_files.append(file)
    
    if missing_files:
        logger.error(f"‚ùå Missing files: {missing_files}")
        return False
    
    logger.info("‚úÖ All required files found")
    return True

def test_event_bus_routes():
    """Test EventBus routes for Phase 57-58"""
    logger.info("üîç Testing EventBus routes...")
    
    try:
        with open("event_bus.json", 'r') as f:
            event_bus = json.load(f)
        
        # Check for Phase 57-58 integration flags
        metadata = event_bus.get("metadata", {})
        if not metadata.get("phase_57_ml_retraining_loop_integrated"):
            logger.error("‚ùå Phase 57 not marked as integrated in EventBus")
            return False
            
        if not metadata.get("phase_58_pattern_learning_engine_integrated"):
            logger.error("‚ùå Phase 58 not marked as integrated in EventBus")
            return False
        
        # Check for required routes
        routes = event_bus.get("routes", [])
        required_routes = [
            "ModelRetrainingTrigger",
            "ModelVersionUpdate", 
            "MLDriftAlert",
            "PatternRecommendation",
            "PatternClusterUpdate"
        ]
        
        found_routes = set()
        for route in routes:
            topic = route.get("topic", "")
            if topic in required_routes:
                found_routes.add(topic)
        
        missing_routes = set(required_routes) - found_routes
        if missing_routes:
            logger.warning(f"‚ö†Ô∏è Missing routes: {missing_routes}")
        
        logger.info("‚úÖ EventBus routes validated")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error testing EventBus routes: {e}")
        return False

def test_telemetry_integration():
    """Test telemetry integration for Phase 57-58"""
    logger.info("üîç Testing telemetry integration...")
    
    try:
        with open("telemetry.json", 'r') as f:
            telemetry = json.load(f)
        
        # Check for Phase 57-58 telemetry events
        events = telemetry.get("events", [])
        
        # Look for ML-related events
        ml_events = [event for event in events if any(term in event.get("topic", "") for term in ["ModelDriftAlert", "PredictionAccuracy", "ModelRetrainingTrigger"])]
        pattern_events = [event for event in events if any(term in event.get("topic", "") for term in ["BacktestResult", "ManualOverride", "StrategyRecommendation"])]
        
        if len(ml_events) < 2:
            logger.warning(f"‚ö†Ô∏è Limited ML telemetry events found: {len(ml_events)}")
        
        if len(pattern_events) < 3:
            logger.warning(f"‚ö†Ô∏è Limited Pattern telemetry events found: {len(pattern_events)}")
        
        logger.info("‚úÖ Telemetry integration validated")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error testing telemetry: {e}")
        return False

def test_build_status():
    """Test build status for Phase 57-58"""
    logger.info("üîç Testing build status...")
    
    try:
        with open("build_status.json", 'r') as f:
            build_status = json.load(f)
        
        # Check for Phase 57-58 completion flags
        phase_57_complete = build_status.get("phase_57_ml_retraining_loop_complete", False)
        phase_58_complete = build_status.get("phase_58_pattern_learning_engine_complete", False)
        
        if not phase_57_complete:
            logger.error("‚ùå Phase 57 not marked as completed")
            return False
            
        if not phase_58_complete:
            logger.error("‚ùå Phase 58 not marked as completed")
            return False
        
        # Check additional phase details
        phase_57_validated = build_status.get("phase_57_ml_retraining_loop_validated", False)
        phase_58_validated = build_status.get("phase_58_pattern_learning_engine_validated", False)
        
        if not phase_57_validated:
            logger.warning("‚ö†Ô∏è Phase 57 not marked as validated")
            
        if not phase_58_validated:
            logger.warning("‚ö†Ô∏è Phase 58 not marked as validated")
        
        logger.info("‚úÖ Build status validated")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error testing build status: {e}")
        return False

def test_module_imports():
    """Test that Phase 57-58 modules can be imported"""
    logger.info("üîç Testing module imports...")
    
    try:
        # Test import of Phase 57
        sys.path.insert(0, os.getcwd())
        
        # Test ML Retraining Loop
        from ml_retraining_loop_phase57 import MLRetrainingLoop
        logger.info("‚úÖ ML Retraining Loop import successful")
        
        # Test Pattern Learning Engine  
        from pattern_learning_engine_phase58 import PatternLearningEngine
        logger.info("‚úÖ Pattern Learning Engine import successful")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error importing modules: {e}")
        return False

def test_module_initialization():
    """Test module initialization without starting full execution"""
    logger.info("üîç Testing module initialization...")
    
    try:
        # Import required modules
        from ml_retraining_loop_phase57 import MLRetrainingLoop
        from pattern_learning_engine_phase58 import PatternLearningEngine


# <!-- @GENESIS_MODULE_END: test_phase57_58_integration_recovered_2 -->


# <!-- @GENESIS_MODULE_START: test_phase57_58_integration_recovered_2 -->
        
        # Test initialization (should not fail)
        logger.info("Initializing ML Retraining Loop...")
        ml_loop = MLRetrainingLoop()
        logger.info("‚úÖ ML Retraining Loop initialized successfully")
        
        logger.info("Initializing Pattern Learning Engine...")
        pattern_engine = PatternLearningEngine()
        logger.info("‚úÖ Pattern Learning Engine initialized successfully")
        
        # Test status methods
        ml_status = ml_loop.get_status()
        pattern_status = pattern_engine.get_status()
        
        logger.info(f"ML Loop Status: {ml_status}")
        logger.info(f"Pattern Engine Status: {pattern_status}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error initializing modules: {e}")
        return False

def test_json_schema_validation():
    """Test JSON schema validation for Phase 57-58 outputs"""
    logger.info("üîç Testing JSON schema validation...")
    
    try:
        # Create test directories if they don't exist
        os.makedirs("models/ml_registry", exist_ok=True)
        os.makedirs("logs/ml_retraining", exist_ok=True)
        os.makedirs("logs/pattern_learning", exist_ok=True)
        
        # Test ML drift log schema
        test_drift_event = {
            "timestamp": datetime.utcnow().isoformat(),
            "drift_reasons": ["accuracy_drift_0.18"],
            "performance_at_drift": {
                "accuracy": 0.75,
                "precision": 0.73,
                "recall": 0.77,
                "f1_score": 0.75
            },
            "data_points_processed": 250
        }
        
        # Test pattern recommendations schema
        test_pattern_recommendation = {
            "timestamp": datetime.utcnow().isoformat(),
            "patterns": [
                {
                    "pattern_id": "technical_pattern_001",
                    "pattern_type": "technical",
                    "success_rate": 0.85,
                    "volatility_compatibility": 0.78,
                    "execution_consistency": 0.82,
                    "rank": 1
                }
            ],
            "total_patterns": 1,
            "learning_confidence": 0.89
        }
        
        # Write test files
        with open("logs/ml_retraining/test_drift_log.json", 'w') as f:
            json.dump(test_drift_event, f, indent=2)
            
        with open("logs/pattern_learning/test_pattern_recommendations.json", 'w') as f:
            json.dump(test_pattern_recommendation, f, indent=2)
        
        logger.info("‚úÖ JSON schema validation passed")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error in JSON schema validation: {e}")
        return False

def generate_test_report():
    """Generate comprehensive test report"""
    logger.info("üìã Generating Phase 57-58 test report...")
    
    test_results = {
        "timestamp": datetime.utcnow().isoformat(),
        "phase": "57-58",
        "title": "ML Retraining Loop + Pattern Learning Engine Integration Test",
        "tests": {}
    }
    
    # Run all tests
    tests = [
        ("file_structure", test_file_structure),
        ("event_bus_routes", test_event_bus_routes), 
        ("telemetry_integration", test_telemetry_integration),
        ("build_status", test_build_status),
        ("module_imports", test_module_imports),
        ("module_initialization", test_module_initialization),
        ("json_schema_validation", test_json_schema_validation)
    ]
    
    passed_tests = 0
    total_tests = len(tests)
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            test_results["tests"][test_name] = {
                "status": "PASS" if result else "FAIL",
                "timestamp": datetime.utcnow().isoformat()
            }
            if result:
                passed_tests += 1
        except Exception as e:
            test_results["tests"][test_name] = {
                "status": "ERROR",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
    
    # Calculate summary
    success_rate = (passed_tests / total_tests) * 100
    test_results["summary"] = {
        "total_tests": total_tests,
        "passed_tests": passed_tests,
        "success_rate": success_rate,
        "overall_status": "PASS" if success_rate >= 85 else "FAIL"
    }
    
    # Update existing test report
    try:
        with open("phase57_58_test_report.json", 'r') as f:
            existing_report = json.load(f)
        
        # Update integration validation section
        existing_report["integration_validation"] = {
            "timestamp": datetime.utcnow().isoformat(),
            "phase_57_58_integration": success_rate >= 85,
            "tests_passed": [name for name, result in test_results["tests"].items() if result["status"] == "PASS"],
            "tests_failed": [name for name, result in test_results["tests"].items() if result["status"] != "PASS"],
            "success_rate": success_rate,
            "status": "PASS" if success_rate >= 85 else "FAIL"
        }
        
        # Save updated report
        with open("phase57_58_test_report.json", 'w') as f:
            json.dump(existing_report, f, indent=2)
            
        logger.info(f"‚úÖ Test report updated - Success rate: {success_rate:.1f}%")
        
    except Exception as e:
        logger.error(f"‚ùå Error updating test report: {e}")
        
        # Create new report
        with open("phase57_58_integration_test_report.json", 'w') as f:
            json.dump(test_results, f, indent=2)
    
    return test_results

def main():
    """Main test execution"""
    logger.info("üöÄ Starting Phase 57-58 Integration Validation Test")
    logger.info("=" * 60)
    
    try:
        # Generate and run tests
        test_results = generate_test_report()
        
        # Print summary
        summary = test_results["summary"]
        logger.info("=" * 60)
        logger.info("üìä TEST SUMMARY")
        logger.info(f"Total Tests: {summary['total_tests']}")
        logger.info(f"Passed Tests: {summary['passed_tests']}")
        logger.info(f"Success Rate: {summary['success_rate']:.1f}%")
        logger.info(f"Overall Status: {summary['overall_status']}")
        
        if summary["overall_status"] == "PASS":
            logger.info("üéâ Phase 57-58 Integration Validation PASSED!")
        else:
            logger.error("‚ùå Phase 57-58 Integration Validation FAILED!")
            
        return summary["overall_status"] == "PASS"
        
    except Exception as e:
        logger.error(f"‚ùå Critical error during testing: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)


def integrate_trading_feedback(model, historical_performance: Dict) -> None:
    """Incorporate real trading feedback into the model"""
    try:
        # Get real trading logs
        real_trades = get_trading_history()
        
        # Extract features and outcomes
        features = []
        outcomes = []
        
        for trade in real_trades:
            # Extract relevant features from the trade
            trade_features = extract_features_from_trade(trade)
            trade_outcome = 1 if trade['profit'] > 0 else 0
            
            features.append(trade_features)
            outcomes.append(trade_outcome)
        
        if len(features) > 10:  # Only update if we have sufficient data
            # Incremental model update
            model.partial_fit(features, outcomes)
            
            # Log update to telemetry
            telemetry.log_event(TelemetryEvent(
                category="ml_optimization", 
                name="model_update", 
                properties={"samples": len(features), "positive_ratio": sum(outcomes)/len(outcomes)}
            ))
            
            # Emit event
            emit_event("model_updated", {
                "model_name": model.__class__.__name__,
                "samples_processed": len(features),
                "timestamp": datetime.now().isoformat()
            })
            
    except Exception as e:
        logging.error(f"Error integrating trading feedback: {str(e)}")
        telemetry.log_event(TelemetryEvent(
            category="error", 
            name="feedback_integration_failed", 
            properties={"error": str(e)}
        ))


def detect_divergence(price_data: list, indicator_data: list, window: int = 10) -> Dict:
    """
    Detect regular and hidden divergences between price and indicator
    
    Args:
        price_data: List of price values (closing prices)
        indicator_data: List of indicator values (e.g., RSI, MACD)
        window: Number of periods to check for divergence
        
    Returns:
        Dictionary with divergence information
    """
    result = {
        "regular_bullish": False,
        "regular_bearish": False,
        "hidden_bullish": False,
        "hidden_bearish": False,
        "strength": 0.0
    }
    
    # Need at least window + 1 periods of data
    if len(price_data) < window + 1 or len(indicator_data) < window + 1:
        return result
        
    # Get the current and historical points
    current_price = price_data[-1]
    previous_price = min(price_data[-window:-1]) if price_data[-1] > price_data[-2] else max(price_data[-window:-1])
    previous_price_idx = price_data[-window:-1].index(previous_price) + len(price_data) - window
    
    current_indicator = indicator_data[-1]
    previous_indicator = indicator_data[previous_price_idx]
    
    # Check for regular divergences
    # Bullish - Lower price lows but higher indicator lows
    if current_price < previous_price and current_indicator > previous_indicator:
        result["regular_bullish"] = True
        result["strength"] = abs((current_indicator - previous_indicator) / previous_indicator)
        
    # Bearish - Higher price highs but lower indicator highs
    elif current_price > previous_price and current_indicator < previous_indicator:
        result["regular_bearish"] = True
        result["strength"] = abs((current_indicator - previous_indicator) / previous_indicator)
    
    # Check for hidden divergences
    # Bullish - Higher price lows but lower indicator lows
    elif current_price > previous_price and current_indicator < previous_indicator:
        result["hidden_bullish"] = True
        result["strength"] = abs((current_indicator - previous_indicator) / previous_indicator)
        
    # Bearish - Lower price highs but higher indicator highs
    elif current_price < previous_price and current_indicator > previous_indicator:
        result["hidden_bearish"] = True
        result["strength"] = abs((current_indicator - previous_indicator) / previous_indicator)
    
    # Emit divergence event if detected
    if any([result["regular_bullish"], result["regular_bearish"], 
            result["hidden_bullish"], result["hidden_bearish"]]):
        emit_event("divergence_detected", {
            "type": next(k for k, v in result.items() if v is True and k != "strength"),
            "strength": result["strength"],
            "symbol": price_data.symbol if hasattr(price_data, "symbol") else "unknown",
            "timestamp": datetime.now().isoformat()
        })
        
    return result

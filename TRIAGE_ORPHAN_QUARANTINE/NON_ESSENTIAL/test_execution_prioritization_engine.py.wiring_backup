# <!-- @GENESIS_MODULE_START: test_execution_prioritization_engine -->

from datetime import datetime\n"""
Test Suite for ExecutionPrioritizationEngine - PHASE 27
GENESIS AI Trading System - ARCHITECT MODE v2.8 COMPLIANT

Tests the Execution Prioritization Layer (EPL) with real signal data processing
"""

import json
import time
import datetime
import unittest
import threading
from collections import defaultdict

# Test with real EventBus integration
try:
    from execution_prioritization_engine import ExecutionPrioritizationEngine, initialize_execution_prioritization_engine
    from hardened_event_bus import get_event_bus, emit_event
    EVENT_BUS_AVAILABLE = True
except ImportError:
    try:
        from execution_prioritization_engine import ExecutionPrioritizationEngine, initialize_execution_prioritization_engine
        from event_bus import get_event_bus, emit_event
        EVENT_BUS_AVAILABLE = True
    except ImportError:
        EVENT_BUS_AVAILABLE = False

class TestExecutionPrioritizationEngine(unittest.TestCase):
    """Comprehensive test suite for ExecutionPrioritizationEngine"""
    
    def setUp(self):
        """Setup test environment"""
        self.engine = ExecutionPrioritizationEngine()
        self.test_results = []
        self.max_test_duration = 10.0  # Maximum test duration in seconds
        
        # Test signal data templates
        self.high_confidence_signal = {
            "signal_id": "test_high_001",
            "symbol": "EURUSD",
            "confidence": 0.92,
            "direction": "BUY",
            "timestamp": time.time(),
            "source": "PatternSignalHarmonizer"
        }
        
        self.medium_confidence_signal = {
            "signal_id": "test_med_001", 
            "symbol": "GBPUSD",
            "confidence": 0.65,
            "direction": "SELL",
            "timestamp": time.time(),
            "source": "PatternSignalHarmonizer"
        }
        
        self.low_confidence_signal = {
            "signal_id": "test_low_001",
            "symbol": "USDJPY",
            "confidence": 0.32,
            "direction": "BUY",
            "timestamp": time.time(),
            "source": "PatternSignalHarmonizer"
        }
    
    def tearDown(self):
        """Cleanup after tests"""
        if hasattr(self, 'engine') and self.engine:
            self.engine.stop()
    
    def test_01_engine_initialization(self):
        """Test engine initialization and basic setup"""
        try:
            # Check initialization
            self.assertIsNotNone(self.engine)
            self.assertTrue(self.engine.processing_active)
            self.assertEqual(self.engine.signals_processed, 0)
            
            # Check queue initialization
            self.assertEqual(self.engine.queue_depths["high"], 0)
            self.assertEqual(self.engine.queue_depths["medium"], 0)
            self.assertEqual(self.engine.queue_depths["low"], 0)
            
            # Check data structures
            self.assertIsInstance(self.engine.symbol_performance, defaultdict)
            self.assertIsInstance(self.engine.priority_calculations, list)
            
            self._add_test_result("ENGINE_INITIALIZATION", True, "Engine initialized successfully")
            
        except Exception as e:
            self._add_test_result("ENGINE_INITIALIZATION", False, f"Initialization failed: {e}")
    
    def test_02_priority_score_calculation(self):
        """Test priority score calculation algorithm"""
        try:
            # Test high confidence signal
            score, tier = self.engine.calculate_priority_score(self.high_confidence_signal)
            self.assertIsInstance(score, float)
            self.assertIn(tier, ["high", "medium", "low"])
            self.assertGreater(score, 0.7)  # High confidence should get high score
            
            # Test medium confidence signal
            score2, tier2 = self.engine.calculate_priority_score(self.medium_confidence_signal)
            self.assertIsInstance(score2, float)
            self.assertIn(tier2, ["high", "medium", "low"])
            
            # Test low confidence signal
            score3, tier3 = self.engine.calculate_priority_score(self.low_confidence_signal)
            self.assertIsInstance(score3, float)
            self.assertIn(tier3, ["high", "medium", "low"])
            
            # High confidence should have higher score than medium
            self.assertGreater(score, score2)
            
            # Medium confidence should have higher score than low
            self.assertGreater(score2, score3)
            
            self._add_test_result("PRIORITY_CALCULATION", True, f"Priority calculation working: High={score:.3f}, Med={score2:.3f}, Low={score3:.3f}")
            
        except Exception as e:
            self._add_test_result("PRIORITY_CALCULATION", False, f"Priority calculation failed: {e}")
    
    def test_03_queue_management(self):
        """Test multi-tier queue management"""
        try:
            # Create prioritized signals
            score1, tier1 = self.engine.calculate_priority_score(self.high_confidence_signal)
            from execution_prioritization_engine import PrioritizedSignal
            
            high_priority_signal = PrioritizedSignal(
                signal_id=self.high_confidence_signal["signal_id"],
                signal_data=self.high_confidence_signal,
                priority_score=score1,
                confidence=self.high_confidence_signal["confidence"],
                queue_tier=tier1,
                created_at=datetime.datetime.now()
            )
            
            # Test adding to queue
            success = self.engine.add_to_priority_queue(high_priority_signal)
            self.assertTrue(success)
            
            # Check queue depth updated
            self.assertGreater(self.engine.queue_depths[tier1], 0)
            
            # Test with multiple signals
            score2, tier2 = self.engine.calculate_priority_score(self.medium_confidence_signal)
            medium_priority_signal = PrioritizedSignal(
                signal_id=self.medium_confidence_signal["signal_id"],
                signal_data=self.medium_confidence_signal,
                priority_score=score2,
                confidence=self.medium_confidence_signal["confidence"],
                queue_tier=tier2,
                created_at=datetime.datetime.now()
            )
            
            success2 = self.engine.add_to_priority_queue(medium_priority_signal)
            self.assertTrue(success2)
            
            self._add_test_result("QUEUE_MANAGEMENT", True, f"Queue management working: {tier1} queue depth={self.engine.queue_depths[tier1]}")
            
        except Exception as e:
            self._add_test_result("QUEUE_MANAGEMENT", False, f"Queue management failed: {e}")
    
    def test_04_signal_harmonized_handling(self):
        """Test handling of SignalHarmonized events"""
        try:
            initial_processed = self.engine.signals_processed
            
            # Handle harmonized signal
            self.engine._handle_harmonized_signal(self.high_confidence_signal)
            
            # Check that signal was processed (queue depth should increase)
            total_queue_depth = sum(self.engine.queue_depths.values())
            self.assertGreater(total_queue_depth, 0)
            
            # Check priority calculations were recorded
            self.assertGreater(len(self.engine.priority_calculations), 0)
            
            latest_calc = self.engine.priority_calculations[-1]
            self.assertEqual(latest_calc["signal_id"], self.high_confidence_signal["signal_id"])
            
            self._add_test_result("HARMONIZED_SIGNAL_HANDLING", True, f"Signal handling working: Queue depth={total_queue_depth}")
            
        except Exception as e:
            self._add_test_result("HARMONIZED_SIGNAL_HANDLING", False, f"Signal handling failed: {e}")
    
    def test_05_execution_result_learning(self):
        """Test performance learning from execution results"""
        try:
            # Simulate execution result
            execution_result = {
                "signal_id": "test_result_001",
                "symbol": "EURUSD",
                "success": True,
                "execution_time": 0.15,
                "timestamp": time.time()
            }
            
            initial_performance = self.engine.symbol_performance.get("EURUSD", 0.5)
            
            # Handle execution result
            self.engine._handle_execution_result(execution_result)
            
            # Check performance was updated
            updated_performance = self.engine.symbol_performance.get("EURUSD")
            self.assertIsNotNone(updated_performance)
            
            # For successful trade, performance should improve
            if execution_result["success"]:
                self.assertGreaterEqual(updated_performance, initial_performance)
            
            # Test failed execution
            failed_result = {
                "signal_id": "test_result_002",
                "symbol": "GBPUSD",
                "success": False,
                "execution_time": 0.25,
                "timestamp": time.time()
            }
            
            self.engine._handle_execution_result(failed_result)
            
            self._add_test_result("EXECUTION_RESULT_LEARNING", True, f"Performance learning working: EURUSD={updated_performance:.3f}")
            
        except Exception as e:
            self._add_test_result("EXECUTION_RESULT_LEARNING", False, f"Performance learning failed: {e}")
    
    def test_06_queue_processing(self):
        """Test queue processing and signal emission"""
        try:
            # Add multiple signals to different queues
            signals_to_process = [
                self.high_confidence_signal,
                self.medium_confidence_signal,
                self.low_confidence_signal
            ]
            
            # Process all signals
            for signal in signals_to_process:
                self.engine._handle_harmonized_signal(signal)
            
            # Let processing thread run briefly
            time.sleep(0.5)
            
            # Check that signals are being processed
            total_processed = self.engine.signals_processed
            
            # Even if EventBus is not available, internal tracking should work
            self.assertGreaterEqual(total_processed, 0)
            
            self._add_test_result("QUEUE_PROCESSING", True, f"Queue processing working: {total_processed} signals processed")
            
        except Exception as e:
            self._add_test_result("QUEUE_PROCESSING", False, f"Queue processing failed: {e}")
    
    def test_07_system_status_reporting(self):
        """Test system status and metrics reporting"""
        try:
            # Handle system status check
            self.engine._handle_system_status_check({})
            
            # Get system status
            status = self.engine.get_system_status()
            
            # Verify status structure
            self.assertIn("processing_active", status)
            self.assertIn("signals_processed", status)
            self.assertIn("queue_depths", status)
            self.assertIn("execution_engines", status)
            
            # Get performance metrics
            metrics = self.engine.get_performance_metrics()
            
            # Verify metrics structure
            self.assertIn("signals_processed", metrics)
            self.assertIn("queue_depths", metrics)
            self.assertIn("priority_distribution", metrics)
            
            self._add_test_result("STATUS_REPORTING", True, f"Status reporting working: {len(status)} status fields, {len(metrics)} metric fields")
            
        except Exception as e:
            self._add_test_result("STATUS_REPORTING", False, f"Status reporting failed: {e}")
    
    def test_08_eventbus_integration(self):
        """Test EventBus integration if available"""
        try:
            if EVENT_BUS_AVAILABLE:
                # Test event bus availability
                self.assertTrue(hasattr(self.engine, '_setup_event_bus'))
                
                # Check that event bus setup was attempted
                self.assertIsNotNone(self.engine.logger)
                
                self._add_test_result("EVENTBUS_INTEGRATION", True, "EventBus integration configured")
            else:
                self._add_test_result("EVENTBUS_INTEGRATION", True, "EventBus not available - graceful degradation working")
                
        except Exception as e:
            self._add_test_result("EVENTBUS_INTEGRATION", False, f"EventBus integration failed: {e}")
    
    def test_09_concurrent_processing(self):
        """Test concurrent signal processing"""
        try:
            # Create multiple threads to execute_live concurrent signals
            def send_signal(signal_data, thread_id):
                signal_copy = signal_data.copy()
                signal_copy["signal_id"] = f"{signal_data['signal_id']}_thread_{thread_id}"
                self.engine._handle_harmonized_signal(signal_copy)
            
            # Launch concurrent threads
            threads = []
            for i in range(5):
                thread = threading.Thread(target=send_signal, args=(self.high_confidence_signal, i))
                threads.append(thread)
                thread.start()
            
            # Wait for all threads to complete
            for thread in threads:
                thread.join(timeout=2.0)
            
            # Check that all signals were processed
            total_queue_depth = sum(self.engine.queue_depths.values())
            self.assertGreaterEqual(total_queue_depth, 0)
            
            # Check priority calculations
            self.assertGreaterEqual(len(self.engine.priority_calculations), 5)
            
            self._add_test_result("CONCURRENT_PROCESSING", True, f"Concurrent processing working: {total_queue_depth} signals queued")
            
        except Exception as e:
            self._add_test_result("CONCURRENT_PROCESSING", False, f"Concurrent processing failed: {e}")
    
    def test_10_performance_validation(self):
        """Test performance and latency requirements"""
        try:
            # Test priority calculation performance
            start_time = time.time()
            
            for i in range(100):
                test_signal = self.high_confidence_signal.copy()
                test_signal["signal_id"] = f"perf_test_{i}"
                self.engine.calculate_priority_score(test_signal)
            
            calculation_time = time.time() - start_time
            avg_calculation_time = calculation_time / 100
            
            # Priority calculation should be fast (< 1ms per signal)
            self.assertLess(avg_calculation_time, 0.001)
            
            # Test memory usage doesn't grow unbounded
            initial_calc_count = len(self.engine.priority_calculations)
            
            # Add many calculations
            for i in range(2000):
                test_signal = self.high_confidence_signal.copy()
                test_signal["signal_id"] = f"memory_test_{i}"
                self.engine.calculate_priority_score(test_signal)
            
            # Should be limited to prevent memory issues
            final_calc_count = len(self.engine.priority_calculations)
            self.assertLessEqual(final_calc_count, 1000)  # Should be limited
            
            self._add_test_result("PERFORMANCE_VALIDATION", True, f"Performance validated: {avg_calculation_time*1000:.2f}ms avg calculation time")
            
        except Exception as e:
            self._add_test_result("PERFORMANCE_VALIDATION", False, f"Performance validation failed: {e}")
    
    def _add_test_result(self, test_name: str, success: bool, message: str):
        """Add test result to results list"""
        self.test_results.append({
            "test_name": test_name,
            "success": success,
            "message": message,
            "timestamp": time.time()
        })
        
        status = "‚úÖ PASSED" if success else "‚ùå FAILED"
        print(f"{status}: {test_name} - {message}")
    
    def generate_test_report(self):
        """Generate comprehensive test report"""
        passed_tests = [r for r in self.test_results if r["success"]]
        failed_tests = [r for r in self.test_results if not r["success"]]
        
        report = {
            "timestamp": datetime.datetime.now().isoformat(),
            "total_tests": len(self.test_results),
            "passed_tests": len(passed_tests),
            "failed_tests": len(failed_tests),
            "success_rate": len(passed_tests) / len(self.test_results) * 100 if self.test_results else 0,
            "test_results": self.test_results,
            "system_status": self.engine.get_system_status() if self.engine else {},
            "performance_metrics": self.engine.get_performance_metrics() if self.engine else {}
        }
        
        return report


def run_phase27_execution_prioritization_tests():
    """Run comprehensive test suite for ExecutionPrioritizationEngine"""
    print("üöÄ Starting PHASE 27 ExecutionPrioritizationEngine Test Suite")
    print("=" * 70)
    
    suite = unittest.TestLoader().loadTestsFromTestCase(TestExecutionPrioritizationEngine)
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # Generate detailed report
    test_instance = TestExecutionPrioritizationEngine()
    test_instance.setUp()
    
    # Run all tests manually to get detailed results
    test_methods = [method for method in dir(test_instance) if method.startswith('test_')]
    
    for method_name in sorted(test_methods):
        try:
            method = getattr(test_instance, method_name)
            method()
        except Exception as e:
            test_instance._add_test_result(method_name, False, f"Test execution failed: {e}")
    
    # Generate final report
    report = test_instance.generate_test_report()
    
    print("\nüéØ PHASE 27 TEST SUMMARY:")
    print(f"Total Tests: {report['total_tests']}")
    print(f"Passed: {report['passed_tests']}")
    print(f"Failed: {report['failed_tests']}")
    print(f"Success Rate: {report['success_rate']:.1f}%")
    
    # Save report
    report_file = f"phase27_epl_test_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\nüìä Detailed report saved to: {report_file}")
    
    test_instance.tearDown()
    
    return report


if __name__ == "__main__":
    # Run the test suite
    report = run_phase27_execution_prioritization_tests()
    
    # Print final status
    if report["success_rate"] >= 80:
        print("\n‚úÖ PHASE 27 ExecutionPrioritizationEngine: TESTS PASSED")
    else:
        print("\n‚ùå PHASE 27 ExecutionPrioritizationEngine: TESTS FAILED")
        print("Check test report for details.")

    def log_state(self):
        """Phase 91 Telemetry Enforcer - Log current module state"""
        state_data = {
            "module": __name__,
            "timestamp": datetime.now().isoformat(),
            "status": "active",
            "phase": "91_telemetry_enforcement"
        }
        if hasattr(self, 'event_bus') and self.event_bus:
            self.event_bus.emit("telemetry", state_data)
        return state_data
        

# <!-- @GENESIS_MODULE_END: test_execution_prioritization_engine -->